{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import swifter\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def lemmatize(word:str) -> str:\n",
    "    lemma = wordnet.morphy(word)\n",
    "    if lemma is None: return word\n",
    "    else: return lemma\n",
    "    \n",
    "def clean_tokens(raw_text:str, stop_words:[str]=[]) -> [str]:\n",
    "    result = raw_text.replace(\"\\\\n\", \" \")\n",
    "    result = [token.lower() for token in tokenizer.tokenize(result) if len(token) > 3]\n",
    "    result = [lemmatize(token) for token in result if re.match(r\"\\D\", token) is not None]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 2.57 seconds.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "df = pd.read_csv(\"in.tsv\",sep=\"\\t\", header=None)\n",
    "df.drop(columns=[1,2,3,5], inplace=True)\n",
    "df.columns = [\"File\", \"Raw\"]\n",
    "df.dropna(inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(\"Took\", round(time.time() -start, 2), \"seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 66.15 seconds.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "df[\"Tokens\"] = df[\"Raw\"].apply(clean_tokens)\n",
    "\n",
    "print(\"Took\", round(time.time() -start, 2), \"seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 7.0 minutes.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "collection = []\n",
    "for idx in df.index:\n",
    "    for word in df.loc[idx, \"Tokens\"]:\n",
    "        if word not in collection:\n",
    "            collection.append(word)\n",
    "            \n",
    "print(\"Took\", (time.time() -start) // 60, \"minutes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dict2Idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 1.57 seconds.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "dictionary = {idx: word for idx, word in enumerate(sorted(collection))}\n",
    "dictionary_rev = {word: idx for idx, word in dictionary.items()}\n",
    "\n",
    "df[\"Idxs\"] = df[\"Tokens\"].apply(lambda x:\n",
    "    list(map(lambda y: dictionary_rev[y], x))\n",
    ")\n",
    "\n",
    "print(\"Took\", round(time.time() -start, 2), \"seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 14.0 minutes.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "df[\"Count\"] = df[\"Idxs\"].apply(lambda x: {idx: x.count(idx) for idx in x})\n",
    "df[\"Count\"] = df[\"Count\"].apply(lambda x:\n",
    "    {k:v for k,v in sorted(x.items(), key=lambda i: i[1])[::-1]}\n",
    ")\n",
    "\n",
    "print(\"Took\", (time.time() -start) // 60, \"minutes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Term Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 24.34 seconds.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "#df[\"TF\"] = df[\"Count\"].apply(lambda x: {k:v/len(x) for k,v in x.items()})\n",
    "\n",
    "df[\"TF\"] = df.apply(lambda x: {\n",
    "    k:v/len(x[\"Tokens\"]) for k,v in x[\"Count\"].items()\n",
    "}, axis=1)\n",
    "\n",
    "print(\"Took\", round(time.time() -start, 2), \"seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 40.18 seconds.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "docs_containing = {idx: sum(map(lambda x: idx in x, df[\"Count\"])) for idx in dictionary}\n",
    "\n",
    "total_docs = len(df)\n",
    "dict_idf = {idx: math.log10(total_docs / docs_containing[idx]) for idx in dictionary}\n",
    "\n",
    "df[\"TFIDF\"] = df[\"TF\"].apply(lambda x: {idx: val * dict_idf[idx] for idx, val in x.items()})\n",
    "\n",
    "print(\"Took\", round(time.time() -start, 2), \"seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top10 Weights"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "start = time.time()\n",
    "\n",
    "df[\"Top10\"] = df[\"TFIDF\"].apply(lambda x: \n",
    "    [idx[0] for idx in sorted(x.items(), key=lambda i: i[1])[::-1][:10]]\n",
    ")\n",
    "\n",
    "print(\"Took\", round(time.time() -start, 2), \"seconds.\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[dictionary[idx] for idx in df.loc[5, \"Top10\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_files() -> None:\n",
    "    with open(\"dictionary.json\", \"w\") as file:\n",
    "        json.dump(dictionary, file)\n",
    "    with open(\"dictionary_rev.json\", \"w\") as file:\n",
    "        json.dump(dictionary_rev, file)\n",
    "    with open(\"docs_containing.json\", \"w\") as file:\n",
    "        json.dump(docs_containing, file)\n",
    "    with open(\"dict_idf.json\", \"w\") as file:\n",
    "        json.dump(dict_idf, file)\n",
    "    df.to_csv(\"tfidf.csv\")\n",
    "\n",
    "save_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2603"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "garbage = []\n",
    "for token in dictionary_rev:\n",
    "    if re.match(r\".*[0_9].*\", token):\n",
    "        garbage.append(token)\n",
    "len(garbage)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
